{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import statsmodels as sm\n",
    "import matplotlib\n",
    "import glob, os    \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats._continuous_distns import _distn_names\n",
    "from cycler import cycler\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (16.0, 12.0)\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "matplotlib.rcParams['axes.prop_cycle'] = cycler(color='bgrcmyk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath=\"C:/Users/kbada/OneDrive/Desktop/github/forgedMachines\"\n",
    "demanddata = pd.concat(map(pd.read_csv, glob.glob(os.path.join(datapath, \"DeliveryData/DeliveryData*.csv\"))))\n",
    "demanddata.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "demanddata.columns\n",
    "# zip\n",
    "zip_codes=pd.concat(map(pd.read_csv, glob.glob(os.path.join(datapath, \"ZIP CODE.csv\"))))\n",
    "zip_codes=list(zip_codes.columns)\n",
    "zip_codes=[int(i) for i in zip_codes]\n",
    "zipdemand=demanddata.groupby(['customer ZIP','scenario']).agg({'quantity':'sum'})\n",
    "zipdemand.reset_index(inplace=True)\n",
    "# commodity demand\n",
    "commodity_demand=demanddata.groupby(['origin facility','customer ZIP','scenario']).agg({'quantity':'sum'})\n",
    "commodity_demand.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ksone', 'kstwobign', 'norm', 'alpha', 'anglit', 'arcsine', 'beta', 'betaprime', 'bradford', 'burr', 'burr12', 'fisk', 'cauchy', 'chi', 'chi2', 'cosine', 'dgamma', 'dweibull', 'expon', 'exponnorm', 'exponweib', 'exponpow', 'fatiguelife', 'foldcauchy', 'f', 'foldnorm', 'weibull_min', 'weibull_max', 'frechet_r', 'frechet_l', 'genlogistic', 'genpareto', 'genexpon', 'genextreme', 'gamma', 'erlang', 'gengamma', 'genhalflogistic', 'gompertz', 'gumbel_r', 'gumbel_l', 'halfcauchy', 'halflogistic', 'halfnorm', 'hypsecant', 'gausshyper', 'invgamma', 'invgauss', 'norminvgauss', 'invweibull', 'johnsonsb', 'johnsonsu', 'laplace', 'levy', 'levy_l', 'levy_stable', 'logistic', 'loggamma', 'loglaplace', 'lognorm', 'gilbrat', 'maxwell', 'mielke', 'kappa4', 'kappa3', 'moyal', 'nakagami', 'ncx2', 'ncf', 't', 'nct', 'pareto', 'lomax', 'pearson3', 'powerlaw', 'powerlognorm', 'powernorm', 'rdist', 'rayleigh', 'reciprocal', 'rice', 'recipinvgauss', 'semicircular', 'skewnorm', 'trapz', 'triang', 'truncexpon', 'truncnorm', 'tukeylambda', 'uniform', 'vonmises', 'vonmises_line', 'wald', 'wrapcauchy', 'gennorm', 'halfgennorm', 'crystalball', 'argus']\n"
     ]
    }
   ],
   "source": [
    "print(_distn_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using website: https://pythonhealthcare.org/2018/05/03/81-distribution-fitting-to-data/\n",
    "import warnings\n",
    "import scipy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def dist_fitter(y):\n",
    "    sc = StandardScaler() \n",
    "    y = y.values.reshape(-1, 1)\n",
    "    sc.fit(y)\n",
    "    y = sc.transform(y)\n",
    "    y = y.flatten()\n",
    "    size = len(y)\n",
    "\n",
    "    dist_names = ['expon', 'lognorm', 'norm']\n",
    "    \n",
    "#     dist_names = ['beta',\n",
    "#                   'expon',\n",
    "#                   'gamma',\n",
    "#                   'lognorm',\n",
    "#                   'norm',\n",
    "#                   'pearson3',\n",
    "#                   'triang',\n",
    "#                   'uniform',\n",
    "#                   'weibull_min', \n",
    "#                   'weibull_max']\n",
    "\n",
    "\n",
    "    chi_square = []\n",
    "    p_values = []\n",
    "\n",
    "    # Set up 50 bins for chi-square test\n",
    "    # Observed data will be approximately evenly distrubuted aross all bins\n",
    "    percentile_bins = np.linspace(0, 100, 10)\n",
    "    percentile_cutoffs = []\n",
    "    percentile_cutoffs = np.percentile(y, percentile_bins, interpolation='midpoint')\n",
    "    \n",
    "    for i in range(len(percentile_cutoffs)-1):\n",
    "        diff = percentile_cutoffs[i+1] - percentile_cutoffs[i]\n",
    "        if diff <= 0:\n",
    "            percentile_cutoffs[i+1] = percentile_cutoffs[i+1] - diff + 0.00000001\n",
    "    observed_frequency, bins = (np.histogram(y, bins=percentile_cutoffs))\n",
    "    cum_observed_frequency = np.cumsum(observed_frequency)\n",
    "\n",
    "    # Loop through candidate distributions\n",
    "\n",
    "    for distribution in dist_names:\n",
    "        # Set up distribution and get fitted distribution parameters\n",
    "        dist = getattr(scipy.stats, distribution)\n",
    "        param = dist.fit(y)\n",
    "\n",
    "        # Obtain the KS test P statistic, round it to 5 decimal places\n",
    "        p = scipy.stats.kstest(y, distribution, args=param)[1]\n",
    "        p = np.around(p, 5)\n",
    "        p_values.append(p)    \n",
    "\n",
    "        # Get expected counts in percentile bins\n",
    "        # This is based on a 'cumulative distrubution function' (cdf)\n",
    "        cdf_fitted = dist.cdf(percentile_cutoffs, *param[:-2], loc=param[-2], \n",
    "                              scale=param[-1])\n",
    "        expected_frequency = []\n",
    "        for bin in range(len(percentile_bins)-1):\n",
    "            expected_cdf_area = cdf_fitted[bin+1] - cdf_fitted[bin]\n",
    "            expected_frequency.append(expected_cdf_area)\n",
    "\n",
    "        # calculate chi-squared\n",
    "        expected_frequency = np.array(expected_frequency) * size\n",
    "        cum_expected_frequency = np.cumsum(expected_frequency)\n",
    "        ss = sum(((cum_expected_frequency - cum_observed_frequency) ** 2) / cum_observed_frequency)\n",
    "        chi_square.append(ss)\n",
    "\n",
    "    # Collate results and sort by goodness of fit (best at top)\n",
    "    results = pd.DataFrame()\n",
    "    results['Distribution'] = dist_names\n",
    "    results['chi_square'] = chi_square\n",
    "    results['p_value'] = p_values\n",
    "    results.sort_values(['chi_square'], inplace=True)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist_params(y, best_distribution):\n",
    "    # Create an empty list to stroe fitted distribution parameters\n",
    "    parameters = []\n",
    "    num_distributions = 1\n",
    "\n",
    "    # Loop through the distributions ot get line fit and paraemters\n",
    "    dist_name = best_distribution['Distribution'].values.item()\n",
    "    dist = getattr(scipy.stats, dist_name)\n",
    "    param = dist.fit(y)\n",
    "    parameters = param\n",
    "\n",
    "    # Store distribution paraemters in a dataframe (this could also be saved)\n",
    "    best_distribution['Distribution parameters'] = [param]\n",
    "\n",
    "    return best_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "num_distributions = 1\n",
    "od_distributions = pd.DataFrame()\n",
    "\n",
    "# Just get first one\n",
    "for zipcode in zip_codes:    \n",
    "    for ori_node in set(commodity_demand['origin facility']):\n",
    "        # Get demand data for each OD-pair\n",
    "        od_demand = commodity_demand.loc[np.logical_and(commodity_demand['customer ZIP'] == zipcode, commodity_demand['origin facility'] == ori_node)]\n",
    "        data = od_demand['quantity']\n",
    "        results = dist_fitter(data)\n",
    "        name = str(ori_node) + \"-\" + str(zipcode)\n",
    "        best_dist = results.iloc[0:num_distributions]\n",
    "        dist_names = results['Distribution'].iloc[0:num_distributions].values.item()\n",
    "        dist_with_params = get_dist_params(data, best_dist)\n",
    "        # Populate dataframe with results\n",
    "        od_distributions = od_distributions.append({'ZIP': zipcode, 'Origin': ori_node, \n",
    "                                                    'Distribution': dist_with_params['Distribution'].iloc[0], \n",
    "                                                    'Parameters': dist_with_params['Distribution parameters'].iloc[0],\n",
    "                                                    'Chi-square': dist_with_params['chi_square'].iloc[0],\n",
    "                                                    'P-value': dist_with_params['p_value'].iloc[0]}, ignore_index=True)\n",
    "od_distributions = od_distributions[['Origin', 'ZIP', 'Distribution', 'Parameters', 'Chi-square', 'P-value']]\n",
    "od_distributions = od_distributions.astype({'ZIP': 'int32'})\n",
    "od_distributions.to_csv('od_distributions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
